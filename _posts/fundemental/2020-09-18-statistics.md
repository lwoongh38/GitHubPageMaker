---
layout: post
current: post
cover:  assets/built/images/bayesian.jpg
navigation: True
title: 통계학의 두 축, 빈도주의와 베이지안 관점에 대해 알아보자
date: 2020-09-18 16:40:00
tags: [fundemental]
class: post-template
subclass: 'post tag-fundemental'
author: woongE
---
#빈도주의 #베이지안 #통계학

{% include fundemental-table-of-contents.html %}

# 통계학의 두 관점, 빈도주의와 베이지안에 대해서 알아보자.

"빈도주의 통계학과 베이지안 관점의 통계학은 무엇이 다른가?" 라는 질문에, 
뭔가 다른 것 같긴 한데 우물쭈물 대답할 말이 마땅히 생각나지 않는 그대는 이 글을 한번쯤 읽어봤으면 좋겠다.
수학, 통계 이런거 아무것도 몰라도 쉽게 이해할 수 있게 말로 장황하게 늘어놓았으니, 겁먹지 말고 시작해보자.


베이지안 통계학이란?
세상의 불확실성을 확률로 설명 가능한  학문이라고 할 수 있다.

확률을 정리하면 크게 `빈도주의`와 `베이지안` 두가지 관점으로 나눌 수 있다.
빈도주의 관점에서는 확률을 **상대적인 빈도의 극한값**으로 나타내는데  이 말은 동일한 일을 무한히 반복했을 때의 빈도를 의미한다. 
빈도주의 관점에서는 동전던지기, 주사위 던지기 등의 무작위 사건의 확률을 설명할 수 있다.

동전을 10번 던졌을 때 6번 앞면이 나온다면 이는 상대적 빈도값이 6/10이 되는 것이고, 
10000번 던졌을 때 5600번 앞면이 나온다면 5600/10000, 횟수를 무한히 늘려나간다면 이 값은 결국 1/2 = 0.5(50%)에 수렴하게 된다. 
이를 앞면이 나올 `확률`이라고 정의하는 것이다.
하지만 빈도주의 관점에서의 확률은 명확한 한계를 가지고 있는데 횟수를 무한히 늘릴 수 없는 사건에 대해서는 확률 계산이 불가능하다는 것이다. 
예를들어 내일 비가 내릴 확률은 상대적인 빈도로는 설명할 수 없다. 같은 조건에서 무한히 반복할 수 없기 때문에 빈도를 계산할 수 없어서 확률을 구할 수가 없다. 이처럼 무작위 사건이 아닌 인식론적 불확실성을 설명하기 위해서 새로운 확률의 정의가 필요성이 생겼고,

그래서 등장한 것이 **베이지안 관점의 확률**이다.
베이지안 관점에서는 확률을 믿음의 정도(degree of belief)로 정의한다. 가령 "내일 비가 내릴 확률은 70%이다"라는 말처럼 어떠한 사건 발생에 대한 믿음의 정도로 확률을 정의하기 때문에, 빈도주의로 설명할 수 없는 훨씬 다양한 상황에 확률을 부여할 수 있다. 때문에 베이지안 관점의 확률을 **불확실성 측정의 도구**라고도 이야기한다.

베이즈정리를 설명해보면,
![image](https://user-images.githubusercontent.com/70134676/93562522-2603a980-f9c1-11ea-8c16-b6b7b8541a1f.png)
P(H) 는 사전확률로 가설 H에 대해서 사전에 우리가 믿고있는 정도(사전지식)를 이야기한다. 
가능도P(D|H)는 가설 H가 주어졌을 때 데이터 D를 지지하는 정도, 즉 데이터D가 관찰될 가능성에 해당한다.
사전확률 P(H) 와 가능도P(D|H)를 곱하여 업데이트된 확률인 사후확률 P(H|D)를 얻는다. 이는 데이터 D가 관찰되었을 때, 가설 H의 확률을 말한다.
![image](https://user-images.githubusercontent.com/70134676/93564809-46356780-f9c5-11ea-84e0-a7c5f3c38142.png)

모집단을 설명하는 가설 H1,H2, H3, H4가 있다고 가정하고, D라는 데이터가 관찰되었을 때, P(H1|D)를 공식으로 유도해보면 다음과 같다.

![image](https://user-images.githubusercontent.com/70134676/93564180-3cf7cb00-f9c4-11ea-9c45-811abd769a77.png)

이제 공식을 설명해보면,
P(H) 는 **사전확률**로 가설 H에 대해서 사전에 우리가 믿고있는 정도(사전지식)를 이야기한다. 
P(D|H)에서 가설 H가 주어졌을 때 데이터 D를 지지하는 정도, 즉 데이터D가 관찰될 가능성에 해당한다.
사전확률 P(H) 와 가능도P(D|H)를 곱하여 업데이트된 확률인 사후확률 P(H|D)를 얻는다. 이는 데이터 D가 관찰되었을 때, 가설 H의 확률을 말한다.

사전확률 H1, H2, H3, H4는 모두 같았지만 D라는 데이터가 관찰되고 난 후, 데이터 D 안에서 H1이 차지하는 비중이 큰 것을 볼 수 있다. 즉, 데이터가 H1을 더 많이 지지하기 때문에 실제 현상에서도 H1이 일어날 확률이 더 높다고 추론할 수 있다.
따라서 사전확률은 동일했지만 데이터 D가 주어졌을 때 H1의 확률은 다른 가설들보다 H1이 더 크게 되는 것이다. 즉 데이터가 관찰되고 나서 사전확률이 사후확률로 **업데이트**된 것이다.
이처럼 베이즈 정리는 `관찰된 데이터`를 토대로 `사전확률`이 어떻게 `사후확률`로 **업데이트** 되는지를 알 수 있게 해주고 데이터는 가능도를 통해 사후확률에 영향을 준다는 것을 알 수 있다.

기존의 빈도주의에 의한 통계는 집단이 어떤 분포를 가지는지를 정의하고 계산을 통한 확률에 대해 이야기한다. 하지만 베이지안 관점에서는 사전확률 즉, 경험이 주어지고 추가적인 정보를 통해 확률을 지속적으로 갱신한다.
현대에 와서 베이지안 관점의 통계가 더 많은 주목을 받는 이유는 경험(데이터)가 나올수록 예측모델이 더 정확해지기 때문인데 데이터가 넘쳐나는 세상이라 그런 듯 하다. 호랑이 등에 날개를 단 격이라고 할까. 추가로 나중에 머신러닝에 대해 공부할 때, 이런 베이즈정리가 많이 쓰인다고 한다. 이번에 공부하면서 머신러닝에 Naive Bayes Clasify라는 이론도 많이 쓰인다는 것을 알게 되었는데 추후 머신러닝에 대해 공부할 때 이에 대해서도 자세히 소개하기로 다짐하며 글을 마친다.
